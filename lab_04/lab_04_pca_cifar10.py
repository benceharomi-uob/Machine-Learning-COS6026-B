# -*- coding: utf-8 -*-
"""lab_04_pca_cifar10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u9ncYDjO_PkxpSo5P--40jhTwwFLiI6q

# CIFAR-10 Dataset

Loading the dataset
"""

from keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

"""Printing the shape of the training and testing images"""

print('Traning data shape:', x_train.shape)
print('Testing data shape:', x_test.shape)

"""Printing the shape of the labels


"""

y_train.shape, y_test.shape

"""Printing the total number of classes (the unique numbers from the train labels)"""

import numpy as np

classes = np.unique(y_train)
nClasses = len(classes)

print('Total number of outputs : ', nClasses)
print('Output classes : ', classes)

"""Creating a dictionary with the names of the classes"""

label_dict = {
 0: 'airplane',
 1: 'automobile',
 2: 'bird',
 3: 'cat',
 4: 'deer',
 5: 'dog',
 6: 'frog',
 7: 'horse',
 8: 'ship',
 9: 'truck',
}

"""Displaying the first image in the training and the test data"""

import matplotlib.pyplot as plt

plt.figure(figsize=[5,5])

# Display the first image in training data
index = 0
plt.subplot(121)
curr_img = np.reshape(x_train[index], (32,32,3))
plt.imshow(curr_img)
curr_label = str(label_dict[y_train[index][0]])
print(plt.title('(Label: {})'.format(curr_label)))

# Display the first image in testing data
index = 0
plt.subplot(122)
curr_img = np.reshape(x_test[index],(32,32,3))
plt.imshow(curr_img)
curr_label = str(label_dict[y_test[index][0]])
print(plt.title('(Label: {})'.format(curr_label)))

"""## Visualizing the data

Checking the minimum and the maximum values of the training images
"""

np.min(x_train),np.max(x_train)

"""Normalizing the pixels between 0 and 1 inclusive"""

x_train_normalized = x_train/np.max(x_train)

np.min(x_train_normalized),np.max(x_train_normalized)

x_train_normalized.shape

"""Reshaping the image dimensions from three to one (flattening the images)"""

from functools import reduce

n_cols = reduce((lambda x, y: x * y), x_train_normalized.shape[1:])
print(n_cols)

x_train_flat = x_train_normalized.reshape(-1, n_cols)
x_train_flat.shape

"""Creating a DataFrame that holds the pixel values of the images with their respective labels in a row-column format

The shape should be (50000, 3073) (3072+1 because of the added labels) 
"""

feat_cols = ['pixel{}'.format(str(i)) for i in range(x_train_flat.shape[1])]

import pandas as pd

df_cifar = pd.DataFrame(x_train_flat,columns=feat_cols)

df_cifar['label'] = y_train
df_cifar.shape

df_cifar.head()

"""PCA - keeping 3 components, then applying `fit_transform` on the training data"""

from sklearn.decomposition import PCA

pca_cifar = PCA(n_components=3)
principalComponents_cifar = pca_cifar.fit_transform(df_cifar.iloc[:,:-1])

"""Converting from a numpy array to a pandas DataFrame"""

principal_cifar_Df = pd.DataFrame(data = principalComponents_cifar
             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])
principal_cifar_Df['y'] = y_train

principal_cifar_Df.head()

"""Printing the **variance**"""

print('Explained variation per principal component: {}'.format(pca_cifar.explained_variance_ratio_))

"""Visualizing the CIFAR-10 data in two-dimensions using PC1 and PC2"""

import seaborn as sns
plt.figure(figsize=(16,10))
sns.scatterplot(
    x='principal component 1', y='principal component 2',
    hue="y",
    palette=sns.color_palette('hls', 10),
    data=principal_cifar_Df,
    legend='full',
    alpha=0.3
)

"""Plot for PC2 vs PC3"""

plt.figure(figsize=(16,10))
sns.scatterplot(
    x='principal component 2', y='principal component 3',
    hue="y",
    palette=sns.color_palette('hls', 10),
    data=principal_cifar_Df,
    legend='full',
    alpha=0.3
)

"""## Speed Up Deep Learning Training using PCA with CIFAR - 10 Dataset

Normalizing and rehaping the test data
"""

x_test_normalized = x_test/np.max(x_test)

x_test_flat = x_test_normalized.reshape(-1, n_cols)
x_test_flat.shape

pca = PCA(0.9)

pca.fit(x_train_flat)

pca.n_components_

train_img_pca = pca.transform(x_train_flat)
test_img_pca = pca.transform(x_test_flat)

from keras.models import Sequential
from keras.layers import Dense
from keras.utils import np_utils
from keras.optimizers import RMSprop

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)

"""Training with PCA"""

batch_size = 128
num_classes = nClasses
num_components = pca.n_components_
epochs = 20

model = Sequential()
model.add(Dense(1024, activation='relu', input_shape=(num_components,)))
model.add(Dense(1024, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(),
              metrics=['accuracy'])

history = model.fit(train_img_pca, y_train,batch_size=batch_size,epochs=epochs,verbose=1,
                    validation_data=(test_img_pca, y_test))

"""Training without PCA"""

batch_size = 128
num_classes = nClasses
num_components = n_cols
epochs = 20

model = Sequential()
model.add(Dense(1024, activation='relu', input_shape=(num_components,)))
model.add(Dense(1024, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(),
              metrics=['accuracy'])

history = model.fit(x_train_flat, y_train,batch_size=batch_size,epochs=epochs,verbose=1,
                    validation_data=(x_test_flat, y_test))